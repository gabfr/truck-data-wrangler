{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Truck Data Wrangler - Streaming part\n",
    "\n",
    "In this notebook we will develop a solution to stream the trucks data using Spark Structured Streaming.\n",
    "\n",
    "First of all, lets get a Spark Session to work on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://67bebe1f2d3c:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Truck Data Wrangler</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f7bf86d3ba8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Truck Data Wrangler\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema\n",
    "\n",
    "After getting the spark session, we'll define the schema of this Structured Streaming process:\n",
    "\n",
    "| Field | Type | Description |\n",
    "| ----- | ---- | ----------- |\n",
    "| `c0` | `integer` | The index key, used just as reference of the order |\n",
    "| `event_type` | `string` | The event type accordingly to the categorization of the data |\n",
    "| `label` | `string` | The label for data segmentation |\n",
    "| `accel_x` | `double` | The X-axis accelerometer value |\n",
    "| `accel_y` | `double` | The Y-axis accelerometer value |\n",
    "| `accel_z` | `double` | The Z-axis accelerometer value |\n",
    "| `gyro_roll` | `double` | The Roll-axis accelerometer value |\n",
    "| `gyro_pitch` | `double` | The Pitch-axis accelerometer value |\n",
    "| `gyro_yaw` | `double` | The Yaw-axis accelerometer value |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "csvSchema = StructType([\n",
    "    StructField(\"c0\", StringType(), True),\n",
    "    StructField(\"event_type\", StringType(), False),\n",
    "    StructField(\"label\", StringType(), False),\n",
    "    StructField(\"timestamp\", LongType(), False),\n",
    "    StructField(\"accel_x\", DoubleType(), False),\n",
    "    StructField(\"accel_y\", DoubleType(), False),\n",
    "    StructField(\"accel_z\", DoubleType(), False),\n",
    "    StructField(\"gyro_roll\", DoubleType(), False),\n",
    "    StructField(\"gyro_pitch\", DoubleType(), False),\n",
    "    StructField(\"gyro_yaw\", DoubleType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c0</th>\n",
       "      <th>event_type</th>\n",
       "      <th>label</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>accel_x</th>\n",
       "      <th>accel_y</th>\n",
       "      <th>accel_z</th>\n",
       "      <th>gyro_roll</th>\n",
       "      <th>gyro_pitch</th>\n",
       "      <th>gyro_yaw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>agressive_bump</td>\n",
       "      <td>0</td>\n",
       "      <td>1550163148318484</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.077898</td>\n",
       "      <td>0.749529</td>\n",
       "      <td>-0.000423</td>\n",
       "      <td>-0.000528</td>\n",
       "      <td>0.000076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>agressive_bump</td>\n",
       "      <td>0</td>\n",
       "      <td>1550163148368484</td>\n",
       "      <td>0.032748</td>\n",
       "      <td>0.077898</td>\n",
       "      <td>0.749353</td>\n",
       "      <td>-0.000423</td>\n",
       "      <td>-0.000528</td>\n",
       "      <td>0.000076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>agressive_bump</td>\n",
       "      <td>0</td>\n",
       "      <td>1550163148418484</td>\n",
       "      <td>0.034472</td>\n",
       "      <td>0.080838</td>\n",
       "      <td>0.749880</td>\n",
       "      <td>-0.000954</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>agressive_bump</td>\n",
       "      <td>0</td>\n",
       "      <td>1550163148468484</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.080838</td>\n",
       "      <td>0.749002</td>\n",
       "      <td>-0.000422</td>\n",
       "      <td>-0.000528</td>\n",
       "      <td>-0.000456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>agressive_bump</td>\n",
       "      <td>0</td>\n",
       "      <td>1550163148518484</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.080838</td>\n",
       "      <td>0.749353</td>\n",
       "      <td>-0.000422</td>\n",
       "      <td>-0.000527</td>\n",
       "      <td>-0.000455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>agressive_bump</td>\n",
       "      <td>0</td>\n",
       "      <td>1550163148568484</td>\n",
       "      <td>0.034472</td>\n",
       "      <td>0.079368</td>\n",
       "      <td>0.750055</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>-0.000527</td>\n",
       "      <td>0.000077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>agressive_bump</td>\n",
       "      <td>0</td>\n",
       "      <td>1550163148618484</td>\n",
       "      <td>0.034472</td>\n",
       "      <td>0.080838</td>\n",
       "      <td>0.748827</td>\n",
       "      <td>-0.000422</td>\n",
       "      <td>-0.000527</td>\n",
       "      <td>0.000077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>agressive_bump</td>\n",
       "      <td>0</td>\n",
       "      <td>1550163148668484</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.079368</td>\n",
       "      <td>0.749529</td>\n",
       "      <td>-0.000421</td>\n",
       "      <td>-0.000526</td>\n",
       "      <td>0.000077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>agressive_bump</td>\n",
       "      <td>0</td>\n",
       "      <td>1550163148718484</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.080838</td>\n",
       "      <td>0.749529</td>\n",
       "      <td>-0.000421</td>\n",
       "      <td>-0.000526</td>\n",
       "      <td>0.000076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>agressive_bump</td>\n",
       "      <td>0</td>\n",
       "      <td>1550163148768484</td>\n",
       "      <td>0.032748</td>\n",
       "      <td>0.079368</td>\n",
       "      <td>0.749178</td>\n",
       "      <td>-0.000421</td>\n",
       "      <td>-0.000526</td>\n",
       "      <td>-0.000455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  c0      event_type label         timestamp   accel_x   accel_y   accel_z  \\\n",
       "0  0  agressive_bump     0  1550163148318484  0.033898  0.077898  0.749529   \n",
       "1  1  agressive_bump     0  1550163148368484  0.032748  0.077898  0.749353   \n",
       "2  2  agressive_bump     0  1550163148418484  0.034472  0.080838  0.749880   \n",
       "3  3  agressive_bump     0  1550163148468484  0.033898  0.080838  0.749002   \n",
       "4  4  agressive_bump     0  1550163148518484  0.033898  0.080838  0.749353   \n",
       "5  5  agressive_bump     0  1550163148568484  0.034472  0.079368  0.750055   \n",
       "6  6  agressive_bump     0  1550163148618484  0.034472  0.080838  0.748827   \n",
       "7  7  agressive_bump     0  1550163148668484  0.033898  0.079368  0.749529   \n",
       "8  8  agressive_bump     0  1550163148718484  0.033898  0.080838  0.749529   \n",
       "9  9  agressive_bump     0  1550163148768484  0.032748  0.079368  0.749178   \n",
       "\n",
       "   gyro_roll  gyro_pitch  gyro_yaw  \n",
       "0  -0.000423   -0.000528  0.000076  \n",
       "1  -0.000423   -0.000528  0.000076  \n",
       "2  -0.000954    0.000004  0.000076  \n",
       "3  -0.000422   -0.000528 -0.000456  \n",
       "4  -0.000422   -0.000527 -0.000455  \n",
       "5   0.000110   -0.000527  0.000077  \n",
       "6  -0.000422   -0.000527  0.000077  \n",
       "7  -0.000421   -0.000526  0.000077  \n",
       "8  -0.000421   -0.000526  0.000076  \n",
       "9  -0.000421   -0.000526 -0.000455  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truck_events_df = spark.read.schema(csvSchema).csv('data/unified.csv', header=True)\n",
    "truck_events_df.createOrReplaceTempView(\"truck_events\")\n",
    "\n",
    "truck_events_df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Processing\n",
    "\n",
    "Now that we tested the schema by loading our default `data/unified.csv`, we have to set the stream processing options and actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPath = 'data/'\n",
    "\n",
    "rawRecords = (\n",
    "    spark\n",
    "        .readStream\n",
    "        .schema(csvSchema)\n",
    "        .option(\"maxFilesPerTrigger\", 1)\n",
    "        .json(inputPath)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating jerk data as stream flow in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- c0: string (nullable = true)\n",
      " |-- event_type: string (nullable = false)\n",
      " |-- label: string (nullable = false)\n",
      " |-- timestamp: long (nullable = false)\n",
      " |-- accel_x: double (nullable = false)\n",
      " |-- accel_y: double (nullable = false)\n",
      " |-- accel_z: double (nullable = false)\n",
      " |-- gyro_roll: double (nullable = false)\n",
      " |-- gyro_pitch: double (nullable = false)\n",
      " |-- gyro_yaw: double (nullable = false)\n",
      " |-- date_timestamp: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "jerk_truck_events_df = rawRecords\n",
    "\n",
    "jerk_truck_events_df = jerk_truck_events_df.withColumn(\n",
    "    \"date_timestamp\",\n",
    "    F.to_date(F.from_unixtime(((col(\"timestamp\") / 1000) / 1000), 'yyyy-MM-dd HH:mm:ss.SSS'))\n",
    ")\n",
    "\n",
    "#column_list = [\"timestamp\", \"event_type\",\"label\"]\n",
    "\n",
    "\n",
    "jerk_truck_events_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explodeJerkColumns(df, epochId):\n",
    "    jerk_truck_events_df = df\n",
    "    \n",
    "    column_list = [\"event_type\",\"label\"]\n",
    "    \n",
    "    win_spec = Window.partitionBy([col(x) for x in column_list]).orderBy(\"timestamp\")\n",
    "\n",
    "    columns_that_needs_latest_values = ['accel_x', 'accel_y', 'accel_z', 'timestamp']\n",
    "\n",
    "    for column_name in columns_that_needs_latest_values:\n",
    "        jerk_truck_events_df = jerk_truck_events_df.withColumn(\"last_\" + column_name, F.lag(col(column_name)).over(win_spec))\n",
    "\n",
    "    # x axis\n",
    "    jerk_truck_events_df = jerk_truck_events_df.withColumn(\n",
    "        \"jerk_x\", \n",
    "        F.when(F.isnull(col(\"last_accel_x\")), 0)\n",
    "         .when(F.isnull(col(\"last_timestamp\")), 0)\n",
    "         .otherwise((col(\"accel_x\") - col(\"last_accel_x\")) / (col(\"timestamp\") - col(\"last_timestamp\")))\n",
    "    )\n",
    "\n",
    "    # y axis\n",
    "    jerk_truck_events_df = jerk_truck_events_df.withColumn(\n",
    "        \"jerk_y\", \n",
    "        F.when(F.isnull(col(\"last_accel_y\")), 0)\n",
    "         .when(F.isnull(col(\"last_timestamp\")), 0)\n",
    "         .otherwise((col(\"accel_y\") - col(\"last_accel_y\")) / (col(\"timestamp\") - col(\"last_timestamp\")))\n",
    "    )\n",
    "\n",
    "    # z axis\n",
    "    jerk_truck_events_df = jerk_truck_events_df.withColumn(\n",
    "        \"jerk_z\", \n",
    "        F.when(F.isnull(col(\"last_accel_z\")), 0)\n",
    "         .when(F.isnull(col(\"last_timestamp\")), 0)\n",
    "         .otherwise((col(\"accel_z\") - col(\"last_accel_z\")) / (col(\"timestamp\") - col(\"last_timestamp\")))\n",
    "    )\n",
    "\n",
    "    # adding the is_accelerating flag\n",
    "    jerk_truck_events_df = jerk_truck_events_df.withColumn(\n",
    "        \"is_accelerating\",\n",
    "        F.when(F.isnull(col(\"jerk_x\")), 0)\n",
    "         .when(col(\"jerk_x\") > 0, 1)\n",
    "         .otherwise(0)\n",
    "    )\n",
    "\n",
    "    # adding the is_breaking flag\n",
    "    jerk_truck_events_df = jerk_truck_events_df.withColumn(\n",
    "        \"is_breaking\",\n",
    "        F.when(F.isnull(col(\"jerk_x\")), 0)\n",
    "         .when(col(\"jerk_x\") < 0, 1)\n",
    "         .otherwise(0)\n",
    "    )\n",
    "    jerk_truck_events_df.createOrReplaceTempView(\"jerked_truck_events\")\n",
    "\n",
    "streamingIn = jerk_truck_events_df \\\n",
    "    .writeStream \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .option(\"checkpointLocation\", \".spark-stream-checkpoint/\") \\\n",
    "    .foreachBatch(explodeJerkColumns) \\\n",
    "    .start(path=inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Table or view not found: jerked_truck_events; line 1 pos 14'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o22.sql.\n: org.apache.spark.sql.AnalysisException: Table or view not found: jerked_truck_events; line 1 pos 14\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:47)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:733)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:685)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:715)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:708)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:708)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:654)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view 'jerked_truck_events' not found in database 'default';\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalog$class.requireTableExists(ExternalCatalog.scala:48)\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.requireTableExists(InMemoryCatalog.scala:45)\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.getTable(InMemoryCatalog.scala:326)\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:138)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:706)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:730)\n\t... 53 more\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2d4b153c8d3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * FROM jerked_truck_events\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Table or view not found: jerked_truck_events; line 1 pos 14'"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM jerked_truck_events\").limit(10).toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
