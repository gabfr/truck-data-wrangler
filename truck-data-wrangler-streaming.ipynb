{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Truck Data Wrangler - Streaming part\n",
    "\n",
    "In this notebook we will develop a solution to stream the trucks data using Spark Structured Streaming.\n",
    "\n",
    "First of all, lets get a Spark Session to work on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Truck Data Wrangler\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema\n",
    "\n",
    "After getting the spark session, we'll define the schema of this Structured Streaming process:\n",
    "\n",
    "| Field | Type | Description |\n",
    "| ----- | ---- | ----------- |\n",
    "| `c0` | `integer` | The index key, used just as reference of the order |\n",
    "| `event_type` | `string` | The event type accordingly to the categorization of the data |\n",
    "| `label` | `string` | The label for data segmentation |\n",
    "| `accel_x` | `double` | The X-axis accelerometer value |\n",
    "| `accel_y` | `double` | The Y-axis accelerometer value |\n",
    "| `accel_z` | `double` | The Z-axis accelerometer value |\n",
    "| `gyro_roll` | `double` | The Roll-axis accelerometer value |\n",
    "| `gyro_pitch` | `double` | The Pitch-axis accelerometer value |\n",
    "| `gyro_yaw` | `double` | The Yaw-axis accelerometer value |\n",
    "\n",
    "## Schema on Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "csvSchema = StructType([\n",
    "    StructField(\"c0\", StringType(), True),\n",
    "    StructField(\"event_type\", StringType(), False),\n",
    "    StructField(\"label\", StringType(), False),\n",
    "    StructField(\"timestamp\", LongType(), False),\n",
    "    StructField(\"accel_x\", DoubleType(), False),\n",
    "    StructField(\"accel_y\", DoubleType(), False),\n",
    "    StructField(\"accel_z\", DoubleType(), False),\n",
    "    StructField(\"gyro_roll\", DoubleType(), False),\n",
    "    StructField(\"gyro_pitch\", DoubleType(), False),\n",
    "    StructField(\"gyro_yaw\", DoubleType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema on TimescaleDB\n",
    "\n",
    "For the database to serve as a read to visualize and query our truck data, we'll go with TimescaleDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/1c/6997288da181277a0c29bc39a5f9143ff20b8c99f2a7d059cfb55163e165/psycopg2-2.8.3.tar.gz (377kB)\n",
      "\u001b[K     |████████████████████████████████| 378kB 2.6MB/s eta 0:00:01\n",
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /opt/conda/bin/python -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-o3eu0p9x/psycopg2/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-o3eu0p9x/psycopg2/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base pip-egg-info\n",
      "         cwd: /tmp/pip-install-o3eu0p9x/psycopg2/\n",
      "    Complete output (23 lines):\n",
      "    running egg_info\n",
      "    creating pip-egg-info/psycopg2.egg-info\n",
      "    writing pip-egg-info/psycopg2.egg-info/PKG-INFO\n",
      "    writing dependency_links to pip-egg-info/psycopg2.egg-info/dependency_links.txt\n",
      "    writing top-level names to pip-egg-info/psycopg2.egg-info/top_level.txt\n",
      "    writing manifest file 'pip-egg-info/psycopg2.egg-info/SOURCES.txt'\n",
      "    \n",
      "    Error: pg_config executable not found.\n",
      "    \n",
      "    pg_config is required to build psycopg2 from source.  Please add the directory\n",
      "    containing pg_config to the $PATH or specify the full executable path with the\n",
      "    option:\n",
      "    \n",
      "        python setup.py build_ext --pg-config /path/to/pg_config build ...\n",
      "    \n",
      "    or with the pg_config option in 'setup.cfg'.\n",
      "    \n",
      "    If you prefer to avoid building psycopg2 from source, please install the PyPI\n",
      "    'psycopg2-binary' package instead.\n",
      "    \n",
      "    For further information please check the 'doc/src/install.rst' file (also at\n",
      "    <http://initd.org/psycopg/docs/install.html>).\n",
      "    \n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[31mERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'psycopg2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3f28c882a6e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpsycopg2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msql_queries\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_jerked_truck_events\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_jerked_truck_events_table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mconfigs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'psycopg2'"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from config import parse_config\n",
    "from sql_queries import create_jerked_truck_events, drop_jerked_truck_events_table\n",
    "\n",
    "configs = parse_config()\n",
    "\n",
    "print(configs['timescaledb']['host'])\n",
    "\n",
    "# connect to default database\n",
    "conn = psycopg2.connect(\"host={} port={} dbname={} user={} password={}\".format( \\\n",
    "    configs['timescaledb']['host'], \\\n",
    "    configs['timescaledb']['port'], \\\n",
    "    configs['timescaledb']['db'], \\\n",
    "    configs['timescaledb']['user'], \\\n",
    "    configs['timescaledb']['password'], \\\n",
    "))\n",
    "conn.set_session(autocommit=True)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# create sparkify database with UTF8 encoding\n",
    "cur.execute(drop_jerked_truck_events_table)\n",
    "cur.execute(create_jerked_truck_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "We will test load the data just to see if the schema is compatible with the stream file source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truck_events_df = spark.read.schema(csvSchema).csv('data/unified.csv', header=True)\n",
    "truck_events_df.createOrReplaceTempView(\"truck_events\")\n",
    "\n",
    "truck_events_df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Processing\n",
    "\n",
    "Now that we tested the schema by loading our default `data/unified.csv`, we have to set the stream processing options and actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPath = 'data/'\n",
    "\n",
    "rawRecords = (\n",
    "    spark\n",
    "        .readStream\n",
    "        .schema(csvSchema)\n",
    "        .option(\"maxFilesPerTrigger\", 1)\n",
    "        .json(inputPath)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating jerk data as stream flow in\n",
    "\n",
    "Essentially, we need to be calculating the jerk values and the flags (is_accelerating, is_breaking, is_turning_right and is_turning_left), however the streaming data frame don't support partitioning/ordering windows with non-time based column types. For that reason we will have to explode that columns in another table using the `forEachBatch` callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "jerk_truck_events_df = rawRecords\n",
    "\n",
    "jerk_truck_events_df = jerk_truck_events_df.withColumn(\n",
    "    \"date_timestamp\",\n",
    "    F.to_date(F.from_unixtime(((col(\"timestamp\") / 1000) / 1000), 'yyyy-MM-dd HH:mm:ss.SSS'))\n",
    ")\n",
    "\n",
    "#column_list = [\"timestamp\", \"event_type\",\"label\"]\n",
    "\n",
    "\n",
    "jerk_truck_events_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explodeJerkColumns(df, epochId):\n",
    "    global configs\n",
    "    jerk_truck_events_df = df\n",
    "    \n",
    "    column_list = [\"event_type\",\"label\"]\n",
    "    \n",
    "    win_spec = Window.partitionBy([col(x) for x in column_list]).orderBy(\"timestamp\")\n",
    "\n",
    "    columns_that_needs_latest_values = ['accel_x', 'accel_y', 'accel_z', 'timestamp']\n",
    "\n",
    "    for column_name in columns_that_needs_latest_values:\n",
    "        jerk_truck_events_df = jerk_truck_events_df.withColumn(\"last_\" + column_name, F.lag(col(column_name)).over(win_spec))\n",
    "\n",
    "    # x axis\n",
    "    jerk_truck_events_df = jerk_truck_events_df.withColumn(\n",
    "        \"jerk_x\", \n",
    "        F.when(F.isnull(col(\"last_accel_x\")), 0)\n",
    "         .when(F.isnull(col(\"last_timestamp\")), 0)\n",
    "         .otherwise((col(\"accel_x\") - col(\"last_accel_x\")) / (col(\"timestamp\") - col(\"last_timestamp\")))\n",
    "    )\n",
    "\n",
    "    # y axis\n",
    "    jerk_truck_events_df = jerk_truck_events_df.withColumn(\n",
    "        \"jerk_y\", \n",
    "        F.when(F.isnull(col(\"last_accel_y\")), 0)\n",
    "         .when(F.isnull(col(\"last_timestamp\")), 0)\n",
    "         .otherwise((col(\"accel_y\") - col(\"last_accel_y\")) / (col(\"timestamp\") - col(\"last_timestamp\")))\n",
    "    )\n",
    "\n",
    "    # z axis\n",
    "    jerk_truck_events_df = jerk_truck_events_df.withColumn(\n",
    "        \"jerk_z\", \n",
    "        F.when(F.isnull(col(\"last_accel_z\")), 0)\n",
    "         .when(F.isnull(col(\"last_timestamp\")), 0)\n",
    "         .otherwise((col(\"accel_z\") - col(\"last_accel_z\")) / (col(\"timestamp\") - col(\"last_timestamp\")))\n",
    "    )\n",
    "\n",
    "    # adding the is_accelerating flag\n",
    "    jerk_truck_events_df = jerk_truck_events_df.withColumn(\n",
    "        \"is_accelerating\",\n",
    "        F.when(F.isnull(col(\"jerk_x\")), 0)\n",
    "         .when(col(\"jerk_x\") > 0, 1)\n",
    "         .otherwise(0)\n",
    "    )\n",
    "\n",
    "    # adding the is_breaking flag\n",
    "    jerk_truck_events_df = jerk_truck_events_df.withColumn(\n",
    "        \"is_breaking\",\n",
    "        F.when(F.isnull(col(\"jerk_x\")), 0)\n",
    "         .when(col(\"jerk_x\") < 0, 1)\n",
    "         .otherwise(0)\n",
    "    )\n",
    "    \n",
    "    dbhost = configs['timescaledb']['host']\n",
    "    dbport = configs['timescaledb']['port']\n",
    "    dbname = configs['timescaledb']['db']\n",
    "    dbuser = configs['timescaledb']['user']\n",
    "    dbpass = configs['timescaledb']['password']\n",
    "    url = \"jdbc:postgresql://\"+dbhost+\":\"+dbport+\"/\"+dbname\n",
    "    properties = {\n",
    "        \"driver\": \"org.postgresql.Driver\",\n",
    "        \"user\": dbuser,\n",
    "        \"password\": dbpass\n",
    "    }\n",
    "\n",
    "    jerk_truck_events_df.write.jdbc(url=url, table=\"jerked_truck_events\", mode=\"append\",\n",
    "                          properties=properties)\n",
    "\n",
    "streamingIn = jerk_truck_events_df \\\n",
    "    .writeStream \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .option(\"checkpointLocation\", \".spark-stream-checkpoint/\") \\\n",
    "    .foreachBatch(explodeJerkColumns) \\\n",
    "    .start(path=inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"SELECT * FROM jerked_truck_events\").limit(10).toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
