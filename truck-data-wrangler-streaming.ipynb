{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Truck Data Wrangler - Streaming part\n",
    "\n",
    "In this notebook we will develop a solution to stream the trucks data using Spark Structured Streaming.\n",
    "\n",
    "First of all, lets get a Spark Session to work on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://45c80b748aee:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Truck Data Wrangler</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa2f5645ba8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Truck Data Wrangler\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema\n",
    "\n",
    "After getting the spark session, we'll define the schema of this Structured Streaming process:\n",
    "\n",
    "| Field | Type | Description |\n",
    "| ----- | ---- | ----------- |\n",
    "| `c0` | `integer` | The index key, used just as reference of the order |\n",
    "| `event_type` | `string` | The event type accordingly to the categorization of the data |\n",
    "| `label` | `string` | The label for data segmentation |\n",
    "| `accel_x` | `double` | The X-axis accelerometer value |\n",
    "| `accel_y` | `double` | The Y-axis accelerometer value |\n",
    "| `accel_z` | `double` | The Z-axis accelerometer value |\n",
    "| `gyro_roll` | `double` | The Roll-axis accelerometer value |\n",
    "| `gyro_pitch` | `double` | The Pitch-axis accelerometer value |\n",
    "| `gyro_yaw` | `double` | The Yaw-axis accelerometer value |\n",
    "\n",
    "## Schema on Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "csvSchema = StructType([\n",
    "    StructField(\"c0\", StringType(), True),\n",
    "    StructField(\"event_type\", StringType(), False),\n",
    "    StructField(\"label\", StringType(), False),\n",
    "    StructField(\"timestamp\", LongType(), False),\n",
    "    StructField(\"accel_x\", DoubleType(), False),\n",
    "    StructField(\"accel_y\", DoubleType(), False),\n",
    "    StructField(\"accel_z\", DoubleType(), False),\n",
    "    StructField(\"gyro_roll\", DoubleType(), False),\n",
    "    StructField(\"gyro_pitch\", DoubleType(), False),\n",
    "    StructField(\"gyro_yaw\", DoubleType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema on TimescaleDB\n",
    "\n",
    "For the database to serve as a read to visualize and query our truck data, we'll go with TimescaleDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2 in /opt/conda/lib/python3.7/site-packages (2.8.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172.17.0.2\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from config import parse_config\n",
    "from sql_queries import create_jerked_truck_events_table, drop_jerked_truck_events_table\n",
    "\n",
    "configs = parse_config()\n",
    "\n",
    "print(configs['timescaledb']['host'])\n",
    "\n",
    "# connect to recreate the database\n",
    "conn = psycopg2.connect(\"host={} port={} user={} password={}\".format( \\\n",
    "    configs['timescaledb']['host'], \\\n",
    "    configs['timescaledb']['port'], \\\n",
    "    configs['timescaledb']['user'], \\\n",
    "    configs['timescaledb']['password'], \\\n",
    "))\n",
    "conn.set_session(autocommit=True)\n",
    "cur = conn.cursor()\n",
    "\n",
    "database_name = configs['timescaledb']['db']\n",
    "\n",
    "#cur.execute(\"DROP DATABASE IF EXISTS {}\".format(database_name))\n",
    "#cur.execute(\"CREATE DATABASE {} WITH ENCODING 'utf8' TEMPLATE template0\".format(database_name))\n",
    "\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to create the tables in the database\n",
    "conn = psycopg2.connect(\"host={} port={} dbname={} user={} password={}\".format( \\\n",
    "    configs['timescaledb']['host'], \\\n",
    "    configs['timescaledb']['port'], \\\n",
    "    database_name, \\\n",
    "    configs['timescaledb']['user'], \\\n",
    "    configs['timescaledb']['password'] \\\n",
    "))\n",
    "conn.set_session(autocommit=True)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# create sparkify database with UTF8 encoding\n",
    "cur.execute(drop_jerked_truck_events_table)\n",
    "cur.execute(create_jerked_truck_events_table)\n",
    "\n",
    "\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "conn = psycopg2.connect(\"host={} port={} dbname={} user={} password={}\".format( \\\n",
    "    configs['timescaledb']['host'], \\\n",
    "    configs['timescaledb']['port'], \\\n",
    "    database_name, \\\n",
    "    configs['timescaledb']['user'], \\\n",
    "    configs['timescaledb']['password'] \\\n",
    "))\n",
    "conn.set_session(autocommit=True)\n",
    "cur = conn.cursor()\n",
    "\n",
    "results = cur.execute(\"SELECT COUNT(*) FROM jerked_truck_events\")\n",
    "\n",
    "print(results)\n",
    "\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "We will test load the data just to see if the schema is compatible with the stream file source.\n",
    "\n",
    "**The code below is commented because it was made to be a playground around the data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import functions as F\n",
    "# from pyspark.sql.functions import col\n",
    "\n",
    "# truck_events_df = spark.read.schema(csvSchema).csv('data/unified.csv', header=True)\n",
    "# truck_events_df = truck_events_df.withColumn(\n",
    "#     \"date_timestamp\",\n",
    "#     F.to_timestamp(F.from_unixtime(((col(\"timestamp\") / 1000) / 1000), 'yyyy-MM-dd HH:mm:ss.SSS'))\n",
    "# )\n",
    "# truck_events_df = (    \n",
    "#     truck_events_df\n",
    "#         .groupBy(col(\"date_timestamp\"), col(\"event_type\"), col(\"label\"), F.window(col(\"date_timestamp\"), \"1 second\"))\n",
    "#         .agg(\n",
    "#             F.mean('accel_x'),\n",
    "#             F.mean('accel_y'),\n",
    "#             F.mean('accel_z'),\n",
    "#             F.mean('gyro_roll'),\n",
    "#             F.mean('gyro_pitch'),\n",
    "#             F.mean('gyro_yaw')\n",
    "#         )\n",
    "#         .withColumnRenamed('avg(accel_x)', 'accel_x')\n",
    "#         .withColumnRenamed('avg(accel_y)', 'accel_y')\n",
    "#         .withColumnRenamed('avg(accel_z)', 'accel_z')\n",
    "#         .withColumnRenamed('avg(gyro_roll)', 'gyro_roll')\n",
    "#         .withColumnRenamed('avg(gyro_pitch)', 'gyro_pitch')\n",
    "#         .withColumnRenamed('avg(gyro_yaw)', 'gyro_yaw')\n",
    "# )\n",
    "\n",
    "# event_types = [\n",
    "#     'aggressive_longitudinal_acceleration',\n",
    "#     'agressive_bump',\n",
    "#     'normal_turn',\n",
    "#     'aggressive_turn',\n",
    "#     'normal_mixed',\n",
    "#     'normal_longitudinal_acceleration'\n",
    "# ]\n",
    "\n",
    "# # truck_events_df.describe(\"gyro_roll\", \"gyro_pitch\", \"gyro_yaw\").show()\n",
    "# from pyspark.sql.functions import mean as _mean, stddev as _stddev, col\n",
    "\n",
    "# df_stats = truck_events_df.select(\n",
    "#     _stddev(col('gyro_yaw')).alias('std')\n",
    "# ).collect()\n",
    "\n",
    "\n",
    "# print(df_stats[0]['std'])\n",
    "\n",
    "# for event_type in event_types:\n",
    "#     print(\"Gonna show for {}\".format(event_type))\n",
    "#     truck_events_df.where(truck_events_df.event_type == event_type).describe(\"gyro_roll\", \"gyro_pitch\", \"gyro_yaw\").show()\n",
    "    \n",
    "# truck_events_df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Processing\n",
    "\n",
    "Now that we tested the schema by loading our default `data/unified.csv`, we have to set the stream processing options and actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPath = '/home/jovyan/workspace/Projects/truck-data-wrangler/data/'\n",
    "\n",
    "rawRecords = (\n",
    "    spark\n",
    "        .readStream\n",
    "        .schema(csvSchema)\n",
    "        .option(\"maxFilesPerTrigger\", 1)\n",
    "        .csv(inputPath, header=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating jerk data as stream flow in\n",
    "\n",
    "Essentially, we need to be calculating the jerk values and the flags (is_accelerating, is_breaking, is_turning_right and is_turning_left), however the streaming data frame don't support partitioning/ordering windows with non-time based column types. For that reason we will have to explode that columns in another table using the `forEachBatch` callback.\n",
    "\n",
    "Moreover, we wil group our data from second to second, and all sensors values will be averaged. This way we can reduce a little bit the noise from them and have a nice graphic to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date_timestamp: timestamp (nullable = true)\n",
      " |-- event_type: string (nullable = false)\n",
      " |-- label: string (nullable = false)\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- accel_x: double (nullable = true)\n",
      " |-- accel_y: double (nullable = true)\n",
      " |-- accel_z: double (nullable = true)\n",
      " |-- gyro_roll: double (nullable = true)\n",
      " |-- gyro_pitch: double (nullable = true)\n",
      " |-- gyro_yaw: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- c0: string (nullable = true)\n",
      " |-- event_type: string (nullable = false)\n",
      " |-- label: string (nullable = false)\n",
      " |-- timestamp: long (nullable = false)\n",
      " |-- accel_x: double (nullable = false)\n",
      " |-- accel_y: double (nullable = false)\n",
      " |-- accel_z: double (nullable = false)\n",
      " |-- gyro_roll: double (nullable = false)\n",
      " |-- gyro_pitch: double (nullable = false)\n",
      " |-- gyro_yaw: double (nullable = false)\n",
      " |-- date_timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "jerk_truck_events_df = rawRecords\n",
    "\n",
    "jerk_truck_events_df = jerk_truck_events_df.withColumn(\n",
    "    \"date_timestamp\",\n",
    "    F.to_timestamp(F.from_unixtime(((col(\"timestamp\") / 1000) / 1000), 'yyyy-MM-dd HH:mm:ss.SSS'))\n",
    ")\n",
    "\n",
    "#column_list = [\"timestamp\", \"event_type\",\"label\"]\n",
    "\n",
    "windowedStreaming = (\n",
    "    jerk_truck_events_df\n",
    "        .groupBy(col(\"date_timestamp\"), col(\"event_type\"), col(\"label\"), F.window(col(\"date_timestamp\"), \"1 second\"))\n",
    "        .agg(\n",
    "            F.mean('accel_x'),\n",
    "            F.mean('accel_y'),\n",
    "            F.mean('accel_z'),\n",
    "            F.mean('gyro_roll'),\n",
    "            F.mean('gyro_pitch'),\n",
    "            F.mean('gyro_yaw')\n",
    "        )\n",
    "        .withColumnRenamed('avg(accel_x)', 'accel_x')\n",
    "        .withColumnRenamed('avg(accel_y)', 'accel_y')\n",
    "        .withColumnRenamed('avg(accel_z)', 'accel_z')\n",
    "        .withColumnRenamed('avg(gyro_roll)', 'gyro_roll')\n",
    "        .withColumnRenamed('avg(gyro_pitch)', 'gyro_pitch')\n",
    "        .withColumnRenamed('avg(gyro_yaw)', 'gyro_yaw')\n",
    ")\n",
    "\n",
    "windowedStreaming.printSchema()\n",
    "\n",
    "jerk_truck_events_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the `forEachBatch` function that we mentioned earlier in this section. Inside this callback you can see that we group over a composed partition. This is because depending upon the `event_type` or `label` we have to treat the sequence differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from pyspark.sql.functions import mean as _mean, stddev as _stddev, col\n",
    "\n",
    "def explodeJerkColumns(df, epochId):\n",
    "    global configs\n",
    "    jerk_truck_events_df = df\n",
    "    \n",
    "    df_stats = jerk_truck_events_df.select(\n",
    "        _stddev(col('gyro_yaw')).alias('std')\n",
    "    ).collect()\n",
    "    \n",
    "    turn_threshold_min = df_stats[0]['std'] * -1\n",
    "    turn_threshold_max = df_stats[0]['std']\n",
    "    \n",
    "    column_list = [\"event_type\",\"label\"]\n",
    "    \n",
    "    jerk_truck_events_df = jerk_truck_events_df.withColumn(\"timestamp\", F.unix_timestamp(col(\"date_timestamp\")))\n",
    "    \n",
    "    win_spec = Window.partitionBy([col(x) for x in column_list]).orderBy(\"timestamp\")\n",
    "\n",
    "    columns_that_needs_latest_values = ['accel_x', 'accel_y', 'accel_z']\n",
    "\n",
    "    for column_name in columns_that_needs_latest_values:\n",
    "        jerk_truck_events_df = jerk_truck_events_df.withColumn(\"last_\" + column_name, F.lag(col(column_name)).over(win_spec))\n",
    "    \n",
    "    # last timestamp before this registry\n",
    "#     jerk_truck_events_df = jerk_truck_events_df.withColumn(\"latest_timestamp\", F.lag(col(\"timestamp\")).over(win_spec))\n",
    "    jerk_truck_events_df = jerk_truck_events_df.withColumn(\n",
    "        \"last_timestamp\", \n",
    "        F.when(F.isnull(F.lag(col(\"date_timestamp\")).over(win_spec)), 0)\n",
    "         .otherwise(F.unix_timestamp(F.lag(col(\"date_timestamp\")).over(win_spec)))\n",
    "    )\n",
    "#     jerk_truck_events_df = jerk_truck_events_df.withColumn(\"date_timestamp\", col(\"timestamp\"))\n",
    "\n",
    "    # x axis\n",
    "    jerk_truck_events_df = jerk_truck_events_df.withColumn(\n",
    "        \"jerk_x\", \n",
    "        F.when(F.isnull(col(\"last_accel_x\")), 0)\n",
    "         .when(F.isnull(col(\"last_timestamp\")), 0)\n",
    "         .otherwise((col(\"accel_x\") - col(\"last_accel_x\")) / (col(\"timestamp\") - col(\"last_timestamp\")))\n",
    "    )\n",
    "\n",
    "    # y axis\n",
    "    jerk_truck_events_df = jerk_truck_events_df.withColumn(\n",
    "        \"jerk_y\", \n",
    "        F.when(F.isnull(col(\"last_accel_y\")), 0)\n",
    "         .when(F.isnull(col(\"last_timestamp\")), 0)\n",
    "         .otherwise((col(\"accel_y\") - col(\"last_accel_y\")) / (col(\"timestamp\") - col(\"last_timestamp\")))\n",
    "    )\n",
    "\n",
    "    # z axis\n",
    "    jerk_truck_events_df = jerk_truck_events_df.withColumn(\n",
    "        \"jerk_z\", \n",
    "        F.when(F.isnull(col(\"last_accel_z\")), 0)\n",
    "         .when(F.isnull(col(\"last_timestamp\")), 0)\n",
    "         .otherwise((col(\"accel_z\") - col(\"last_accel_z\")) / (col(\"timestamp\") - col(\"last_timestamp\")))\n",
    "    )\n",
    "\n",
    "    # adding the is_accelerating flag\n",
    "    jerk_truck_events_df = jerk_truck_events_df.withColumn(\n",
    "        \"is_accelerating\",\n",
    "        F.when(F.isnull(col(\"jerk_x\")), 0)\n",
    "         .when(col(\"jerk_x\") > 0, 1)\n",
    "         .otherwise(0)\n",
    "    )\n",
    "\n",
    "    # adding the is_breaking flag\n",
    "    jerk_truck_events_df = jerk_truck_events_df.withColumn(\n",
    "        \"is_breaking\",\n",
    "        F.when(F.isnull(col(\"jerk_x\")), 0)\n",
    "         .when(col(\"jerk_x\") < 0, 1)\n",
    "         .otherwise(0)\n",
    "    )\n",
    "    \n",
    "    # adding the is_turning_left flag\n",
    "    jerk_truck_events_df = jerk_truck_events_df.withColumn(\n",
    "        \"is_turning_left\",\n",
    "        F.when(col(\"gyro_yaw\") <= turn_threshold_min, 1)\n",
    "         .otherwise(0)\n",
    "    )\n",
    "    \n",
    "    # adding the is_turning_right flag\n",
    "    jerk_truck_events_df = jerk_truck_events_df.withColumn(\n",
    "        \"is_turning_right\",\n",
    "        F.when(col(\"gyro_yaw\") >= turn_threshold_max, 1)\n",
    "         .otherwise(0)\n",
    "    )\n",
    "    \n",
    "    dbhost = configs['timescaledb']['host']\n",
    "    dbport = configs['timescaledb']['port']\n",
    "    dbname = configs['timescaledb']['db']\n",
    "    dbuser = configs['timescaledb']['user']\n",
    "    dbpass = configs['timescaledb']['password']\n",
    "    url = \"jdbc:postgresql://\"+dbhost+\":\"+dbport+\"/\"+dbname\n",
    "    properties = {\n",
    "        \"driver\": \"org.postgresql.Driver\",\n",
    "        \"user\": dbuser,\n",
    "        \"password\": dbpass\n",
    "    }\n",
    "    \n",
    "    jerk_truck_events_df = jerk_truck_events_df[[\n",
    "        'date_timestamp',\n",
    "        'event_type',\n",
    "        'label',\n",
    "        'accel_x',\n",
    "        'accel_y',\n",
    "        'accel_z',\n",
    "        'gyro_roll',\n",
    "        'gyro_pitch',\n",
    "        'gyro_yaw',\n",
    "        'last_timestamp',\n",
    "        'last_accel_x',\n",
    "        'last_accel_y',\n",
    "        'last_accel_z',\n",
    "        'jerk_x',\n",
    "        'jerk_y',\n",
    "        'jerk_z',\n",
    "        'is_accelerating',\n",
    "        'is_breaking',\n",
    "        'is_turning_left',\n",
    "        'is_turning_right'\n",
    "    ]]\n",
    "\n",
    "    jerk_truck_events_df.write.jdbc(url=url, table=\"jerked_truck_events\", mode=\"append\",\n",
    "                          properties=properties)\n",
    "\n",
    "streamingIn = windowedStreaming \\\n",
    "    .writeStream \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .option(\"checkpointLocation\", \".spark-stream-checkpoint/\") \\\n",
    "    .foreachBatch(explodeJerkColumns) \\\n",
    "    .start(path=inputPath) \\\n",
    "    .awaitTermination()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
