{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Truck Data Wrangler - Streaming part\n",
    "\n",
    "In this notebook we will develop a solution to stream the trucks data using Spark Structured Streaming.\n",
    "\n",
    "First of all, lets get a Spark Session to work on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://19f5c8d70091:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Truck Data Wrangler</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f66f074bb00>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Truck Data Wrangler\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema\n",
    "\n",
    "After getting the spark session, we'll define the schema of this Structured Streaming process:\n",
    "\n",
    "| Field | Type | Description |\n",
    "| ----- | ---- | ----------- |\n",
    "| `c0` | `integer` | The index key, used just as reference of the order |\n",
    "| `event_type` | `string` | The event type accordingly to the categorization of the data |\n",
    "| `label` | `string` | The label for data segmentation |\n",
    "| `accel_x` | `double` | The X-axis accelerometer value |\n",
    "| `accel_y` | `double` | The Y-axis accelerometer value |\n",
    "| `accel_z` | `double` | The Z-axis accelerometer value |\n",
    "| `gyro_roll` | `double` | The Roll-axis accelerometer value |\n",
    "| `gyro_pitch` | `double` | The Pitch-axis accelerometer value |\n",
    "| `gyro_yaw` | `double` | The Yaw-axis accelerometer value |\n",
    "\n",
    "## Schema on Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "csvSchema = StructType([\n",
    "    StructField(\"c0\", StringType(), True),\n",
    "    StructField(\"event_type\", StringType(), False),\n",
    "    StructField(\"label\", StringType(), False),\n",
    "    StructField(\"timestamp\", LongType(), False),\n",
    "    StructField(\"accel_x\", DoubleType(), False),\n",
    "    StructField(\"accel_y\", DoubleType(), False),\n",
    "    StructField(\"accel_z\", DoubleType(), False),\n",
    "    StructField(\"gyro_roll\", DoubleType(), False),\n",
    "    StructField(\"gyro_pitch\", DoubleType(), False),\n",
    "    StructField(\"gyro_yaw\", DoubleType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema on TimescaleDB\n",
    "\n",
    "For the database to serve as a read to visualize and query our truck data, we'll go with TimescaleDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172.17.0.2\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from config import parse_config\n",
    "from sql_queries import create_jerked_truck_events_table, drop_jerked_truck_events_table\n",
    "\n",
    "configs = parse_config()\n",
    "\n",
    "print(configs['timescaledb']['host'])\n",
    "\n",
    "# connect to recreate the database\n",
    "conn = psycopg2.connect(\"host={} port={} user={} password={}\".format( \\\n",
    "    configs['timescaledb']['host'], \\\n",
    "    configs['timescaledb']['port'], \\\n",
    "    configs['timescaledb']['user'], \\\n",
    "    configs['timescaledb']['password'], \\\n",
    "))\n",
    "conn.set_session(autocommit=True)\n",
    "cur = conn.cursor()\n",
    "\n",
    "database_name = configs['timescaledb']['db']\n",
    "\n",
    "#cur.execute(\"DROP DATABASE IF EXISTS {}\".format(database_name))\n",
    "#cur.execute(\"CREATE DATABASE {} WITH ENCODING 'utf8' TEMPLATE template0\".format(database_name))\n",
    "\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to create the tables in the database\n",
    "conn = psycopg2.connect(\"host={} port={} dbname={} user={} password={}\".format( \\\n",
    "    configs['timescaledb']['host'], \\\n",
    "    configs['timescaledb']['port'], \\\n",
    "    database_name, \\\n",
    "    configs['timescaledb']['user'], \\\n",
    "    configs['timescaledb']['password'] \\\n",
    "))\n",
    "conn.set_session(autocommit=True)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# create sparkify database with UTF8 encoding\n",
    "cur.execute(drop_jerked_truck_events_table)\n",
    "cur.execute(create_jerked_truck_events_table)\n",
    "\n",
    "\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "conn = psycopg2.connect(\"host={} port={} dbname={} user={} password={}\".format( \\\n",
    "    configs['timescaledb']['host'], \\\n",
    "    configs['timescaledb']['port'], \\\n",
    "    database_name, \\\n",
    "    configs['timescaledb']['user'], \\\n",
    "    configs['timescaledb']['password'] \\\n",
    "))\n",
    "conn.set_session(autocommit=True)\n",
    "cur = conn.cursor()\n",
    "\n",
    "results = cur.execute(\"SELECT COUNT(*) FROM jerked_truck_events\")\n",
    "\n",
    "print(results)\n",
    "\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "We will test load the data just to see if the schema is compatible with the stream file source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#truck_events_df = spark.read.schema(csvSchema).csv('data/unified.csv', header=True)\n",
    "#truck_events_df.createOrReplaceTempView(\"truck_events\")\n",
    "\n",
    "#truck_events_df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Processing\n",
    "\n",
    "Now that we tested the schema by loading our default `data/unified.csv`, we have to set the stream processing options and actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPath = '/home/jovyan/workspace/Projects/truck-data-wrangler/data/'\n",
    "\n",
    "rawRecords = (\n",
    "    spark\n",
    "        .readStream\n",
    "        .schema(csvSchema)\n",
    "        .option(\"maxFilesPerTrigger\", 1)\n",
    "        .csv(inputPath, header=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating jerk data as stream flow in\n",
    "\n",
    "Essentially, we need to be calculating the jerk values and the flags (is_accelerating, is_breaking, is_turning_right and is_turning_left), however the streaming data frame don't support partitioning/ordering windows with non-time based column types. For that reason we will have to explode that columns in another table using the `forEachBatch` callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- c0: string (nullable = true)\n",
      " |-- event_type: string (nullable = false)\n",
      " |-- label: string (nullable = false)\n",
      " |-- timestamp: long (nullable = false)\n",
      " |-- accel_x: double (nullable = false)\n",
      " |-- accel_y: double (nullable = false)\n",
      " |-- accel_z: double (nullable = false)\n",
      " |-- gyro_roll: double (nullable = false)\n",
      " |-- gyro_pitch: double (nullable = false)\n",
      " |-- gyro_yaw: double (nullable = false)\n",
      " |-- date_timestamp: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "jerk_truck_events_df = rawRecords\n",
    "\n",
    "jerk_truck_events_df = jerk_truck_events_df.withColumn(\n",
    "    \"date_timestamp\",\n",
    "    F.to_date(F.from_unixtime(((col(\"timestamp\") / 1000) / 1000), 'yyyy-MM-dd HH:mm:ss.SSS'))\n",
    ")\n",
    "\n",
    "#column_list = [\"timestamp\", \"event_type\",\"label\"]\n",
    "\n",
    "\n",
    "jerk_truck_events_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "StreamingQueryException",
     "evalue": "'An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\\n  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 2381, in _call_proxy\\n    return_value = getattr(self.pool[obj_id], method)(*params)\\n  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 191, in call\\n    raise e\\n  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 188, in call\\n    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\\n  File \"<ipython-input-9-a9f0f6ce4cd6>\", line 90, in explodeJerkColumns\\n    properties=properties)\\n  File \"/usr/local/spark/python/pyspark/sql/readwriter.py\", line 982, in jdbc\\n    self.mode(mode)._jwrite.jdbc(url, table, jprop)\\n  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\\n    answer, self.gateway_client, self.target_id, self.name)\\n  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 63, in deco\\n    return f(*a, **kw)\\n  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\\n    format(target_id, \".\", name), value)\\npy4j.protocol.Py4JJavaError: An error occurred while calling o163.jdbc.\\n: java.lang.ClassNotFoundException: org.postgresql.Driver\\n\\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\\n\\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\\n\\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:45)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$5.apply(JDBCOptions.scala:99)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$5.apply(JDBCOptions.scala:99)\\n\\tat scala.Option.foreach(Option.scala:257)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:99)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:197)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:201)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:45)\\n\\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\\n\\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\\n\\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\\n\\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\\n\\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\\n\\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\\n\\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\\n\\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\\n\\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\\n\\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\\n\\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\\n\\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\\n\\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\\n\\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\\n\\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\\n\\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\\n\\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\\n\\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\\n\\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:515)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n\\n\\n=== Streaming Query ===\\nIdentifier: [id = 452fa5a5-589b-4bc0-8800-c6bd8383d6a0, runId = be592de2-6cef-40d7-94a6-00a2389e6061]\\nCurrent Committed Offsets: {}\\nCurrent Available Offsets: {FileStreamSource[file:/home/jovyan/workspace/Projects/truck-data-wrangler/data]: {\"logOffset\":0}}\\n\\nCurrent State: ACTIVE\\nThread State: RUNNABLE\\n\\nLogical Plan:\\nProject [c0#0, event_type#1, label#2, timestamp#3L, accel_x#4, accel_y#5, accel_z#6, gyro_roll#7, gyro_pitch#8, gyro_yaw#9, to_date(from_unixtime(((\\'timestamp / 1000) / 1000), yyyy-MM-dd HH:mm:ss.SSS, None), None) AS date_timestamp#20]\\n+- StreamingExecutionRelation FileStreamSource[file:/home/jovyan/workspace/Projects/truck-data-wrangler/data], [c0#0, event_type#1, label#2, timestamp#3L, accel_x#4, accel_y#5, accel_z#6, gyro_roll#7, gyro_pitch#8, gyro_yaw#9]\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o52.awaitTermination.\n: org.apache.spark.sql.streaming.StreamingQueryException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 2381, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 191, in call\n    raise e\n  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 188, in call\n    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\n  File \"<ipython-input-9-a9f0f6ce4cd6>\", line 90, in explodeJerkColumns\n    properties=properties)\n  File \"/usr/local/spark/python/pyspark/sql/readwriter.py\", line 982, in jdbc\n    self.mode(mode)._jwrite.jdbc(url, table, jprop)\n  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n    format(target_id, \".\", name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling o163.jdbc.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:45)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$5.apply(JDBCOptions.scala:99)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$5.apply(JDBCOptions.scala:99)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:99)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:197)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:201)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:45)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:515)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n=== Streaming Query ===\nIdentifier: [id = 452fa5a5-589b-4bc0-8800-c6bd8383d6a0, runId = be592de2-6cef-40d7-94a6-00a2389e6061]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {FileStreamSource[file:/home/jovyan/workspace/Projects/truck-data-wrangler/data]: {\"logOffset\":0}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nProject [c0#0, event_type#1, label#2, timestamp#3L, accel_x#4, accel_y#5, accel_z#6, gyro_roll#7, gyro_pitch#8, gyro_yaw#9, to_date(from_unixtime((('timestamp / 1000) / 1000), yyyy-MM-dd HH:mm:ss.SSS, None), None) AS date_timestamp#20]\n+- StreamingExecutionRelation FileStreamSource[file:/home/jovyan/workspace/Projects/truck-data-wrangler/data], [c0#0, event_type#1, label#2, timestamp#3L, accel_x#4, accel_y#5, accel_z#6, gyro_roll#7, gyro_pitch#8, gyro_yaw#9]\n\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:297)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:193)\nCaused by: py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 2381, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 191, in call\n    raise e\n  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 188, in call\n    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\n  File \"<ipython-input-9-a9f0f6ce4cd6>\", line 90, in explodeJerkColumns\n    properties=properties)\n  File \"/usr/local/spark/python/pyspark/sql/readwriter.py\", line 982, in jdbc\n    self.mode(mode)._jwrite.jdbc(url, table, jprop)\n  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n    format(target_id, \".\", name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling o163.jdbc.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:45)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$5.apply(JDBCOptions.scala:99)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$5.apply(JDBCOptions.scala:99)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:99)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:197)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:201)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:45)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:515)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n\tat py4j.Protocol.getReturnValue(Protocol.java:473)\n\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108)\n\tat com.sun.proxy.$Proxy17.call(Unknown Source)\n\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$$anonfun$callForeachBatch$1.apply(ForeachBatchSink.scala:55)\n\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$$anonfun$callForeachBatch$1.apply(ForeachBatchSink.scala:55)\n\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:35)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch$5$$anonfun$apply$17.apply(MicroBatchExecution.scala:537)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch$5.apply(MicroBatchExecution.scala:535)\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:351)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:58)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch(MicroBatchExecution.scala:534)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply$mcV$sp(MicroBatchExecution.scala:198)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:166)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:166)\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:351)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:58)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1.apply$mcZ$sp(MicroBatchExecution.scala:166)\n\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:56)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:160)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:281)\n\t... 1 more\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a9f0f6ce4cd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"checkpointLocation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".spark-stream-checkpoint/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mforeachBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexplodeJerkColumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.streaming.StreamingQueryException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mStreamingQueryException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.execution.QueryExecutionException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: 'An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\\n  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 2381, in _call_proxy\\n    return_value = getattr(self.pool[obj_id], method)(*params)\\n  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 191, in call\\n    raise e\\n  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 188, in call\\n    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\\n  File \"<ipython-input-9-a9f0f6ce4cd6>\", line 90, in explodeJerkColumns\\n    properties=properties)\\n  File \"/usr/local/spark/python/pyspark/sql/readwriter.py\", line 982, in jdbc\\n    self.mode(mode)._jwrite.jdbc(url, table, jprop)\\n  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\\n    answer, self.gateway_client, self.target_id, self.name)\\n  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 63, in deco\\n    return f(*a, **kw)\\n  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\\n    format(target_id, \".\", name), value)\\npy4j.protocol.Py4JJavaError: An error occurred while calling o163.jdbc.\\n: java.lang.ClassNotFoundException: org.postgresql.Driver\\n\\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\\n\\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\\n\\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:45)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$5.apply(JDBCOptions.scala:99)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$5.apply(JDBCOptions.scala:99)\\n\\tat scala.Option.foreach(Option.scala:257)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:99)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:197)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:201)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:45)\\n\\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\\n\\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\\n\\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\\n\\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\\n\\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\\n\\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\\n\\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\\n\\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\\n\\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\\n\\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\\n\\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\\n\\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\\n\\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\\n\\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\\n\\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\\n\\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\\n\\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\\n\\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\\n\\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:515)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n\\n\\n=== Streaming Query ===\\nIdentifier: [id = 452fa5a5-589b-4bc0-8800-c6bd8383d6a0, runId = be592de2-6cef-40d7-94a6-00a2389e6061]\\nCurrent Committed Offsets: {}\\nCurrent Available Offsets: {FileStreamSource[file:/home/jovyan/workspace/Projects/truck-data-wrangler/data]: {\"logOffset\":0}}\\n\\nCurrent State: ACTIVE\\nThread State: RUNNABLE\\n\\nLogical Plan:\\nProject [c0#0, event_type#1, label#2, timestamp#3L, accel_x#4, accel_y#5, accel_z#6, gyro_roll#7, gyro_pitch#8, gyro_yaw#9, to_date(from_unixtime(((\\'timestamp / 1000) / 1000), yyyy-MM-dd HH:mm:ss.SSS, None), None) AS date_timestamp#20]\\n+- StreamingExecutionRelation FileStreamSource[file:/home/jovyan/workspace/Projects/truck-data-wrangler/data], [c0#0, event_type#1, label#2, timestamp#3L, accel_x#4, accel_y#5, accel_z#6, gyro_roll#7, gyro_pitch#8, gyro_yaw#9]\\n'"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "\n",
    "def explodeJerkColumns(df, epochId):\n",
    "    global configs\n",
    "    jerk_truck_events_df = df\n",
    "    \n",
    "    column_list = [\"event_type\",\"label\"]\n",
    "    \n",
    "    win_spec = Window.partitionBy([col(x) for x in column_list]).orderBy(\"timestamp\")\n",
    "\n",
    "    columns_that_needs_latest_values = ['accel_x', 'accel_y', 'accel_z', 'timestamp']\n",
    "\n",
    "    for column_name in columns_that_needs_latest_values:\n",
    "        jerk_truck_events_df = jerk_truck_events_df.withColumn(\"last_\" + column_name, F.lag(col(column_name)).over(win_spec))\n",
    "\n",
    "    # x axis\n",
    "    jerk_truck_events_df = jerk_truck_events_df.withColumn(\n",
    "        \"jerk_x\", \n",
    "        F.when(F.isnull(col(\"last_accel_x\")), 0)\n",
    "         .when(F.isnull(col(\"last_timestamp\")), 0)\n",
    "         .otherwise((col(\"accel_x\") - col(\"last_accel_x\")) / (col(\"timestamp\") - col(\"last_timestamp\")))\n",
    "    )\n",
    "\n",
    "    # y axis\n",
    "    jerk_truck_events_df = jerk_truck_events_df.withColumn(\n",
    "        \"jerk_y\", \n",
    "        F.when(F.isnull(col(\"last_accel_y\")), 0)\n",
    "         .when(F.isnull(col(\"last_timestamp\")), 0)\n",
    "         .otherwise((col(\"accel_y\") - col(\"last_accel_y\")) / (col(\"timestamp\") - col(\"last_timestamp\")))\n",
    "    )\n",
    "\n",
    "    # z axis\n",
    "    jerk_truck_events_df = jerk_truck_events_df.withColumn(\n",
    "        \"jerk_z\", \n",
    "        F.when(F.isnull(col(\"last_accel_z\")), 0)\n",
    "         .when(F.isnull(col(\"last_timestamp\")), 0)\n",
    "         .otherwise((col(\"accel_z\") - col(\"last_accel_z\")) / (col(\"timestamp\") - col(\"last_timestamp\")))\n",
    "    )\n",
    "\n",
    "    # adding the is_accelerating flag\n",
    "    jerk_truck_events_df = jerk_truck_events_df.withColumn(\n",
    "        \"is_accelerating\",\n",
    "        F.when(F.isnull(col(\"jerk_x\")), 0)\n",
    "         .when(col(\"jerk_x\") > 0, 1)\n",
    "         .otherwise(0)\n",
    "    )\n",
    "\n",
    "    # adding the is_breaking flag\n",
    "    jerk_truck_events_df = jerk_truck_events_df.withColumn(\n",
    "        \"is_breaking\",\n",
    "        F.when(F.isnull(col(\"jerk_x\")), 0)\n",
    "         .when(col(\"jerk_x\") < 0, 1)\n",
    "         .otherwise(0)\n",
    "    )\n",
    "    \n",
    "    dbhost = configs['timescaledb']['host']\n",
    "    dbport = configs['timescaledb']['port']\n",
    "    dbname = configs['timescaledb']['db']\n",
    "    dbuser = configs['timescaledb']['user']\n",
    "    dbpass = configs['timescaledb']['password']\n",
    "    url = \"jdbc:postgresql://\"+dbhost+\":\"+dbport+\"/\"+dbname\n",
    "    properties = {\n",
    "        \"driver\": \"org.postgresql.Driver\",\n",
    "        \"user\": dbuser,\n",
    "        \"password\": dbpass\n",
    "    }\n",
    "    \n",
    "    jerk_truck_events_df = jerk_truck_events_df[[\n",
    "        'date_timestamp',\n",
    "        'event_type',\n",
    "        'label',\n",
    "        'accel_x',\n",
    "        'accel_y',\n",
    "        'accel_z',\n",
    "        'gyro_roll',\n",
    "        'gyro_pitch',\n",
    "        'gyro_yaw',\n",
    "        'last_timestamp',\n",
    "        'last_accel_x',\n",
    "        'last_accel_y',\n",
    "        'last_accel_z',\n",
    "        'jerk_x',\n",
    "        'jerk_y',\n",
    "        'jerk_z',\n",
    "        'is_accelerating',\n",
    "        'is_breaking'\n",
    "    ]]\n",
    "\n",
    "    jerk_truck_events_df.write.jdbc(url=url, table=\"jerked_truck_events\", mode=\"append\",\n",
    "                          properties=properties)\n",
    "\n",
    "streamingIn = jerk_truck_events_df \\\n",
    "    .writeStream \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .option(\"checkpointLocation\", \".spark-stream-checkpoint/\") \\\n",
    "    .foreachBatch(explodeJerkColumns) \\\n",
    "    .start(path=inputPath) \\\n",
    "    .awaitTermination()\n",
    "\n",
    "#sleep(30)  # wait a bit for computation to start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#spark.sql(\"SELECT * FROM jerked_truck_events\").limit(10).toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
