{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Truck Data Wrangler - Streaming part\n",
    "\n",
    "In this notebook we will develop a solution to stream the trucks data using Spark Structured Streaming.\n",
    "\n",
    "First of all, lets get a Spark Session to work on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://45c80b748aee:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Truck Data Wrangler</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f91c00f3c88>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Truck Data Wrangler\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema\n",
    "\n",
    "After getting the spark session, we'll define the schema of this Structured Streaming process:\n",
    "\n",
    "| Field | Type | Description |\n",
    "| ----- | ---- | ----------- |\n",
    "| `c0` | `integer` | The index key, used just as reference of the order |\n",
    "| `event_type` | `string` | The event type accordingly to the categorization of the data |\n",
    "| `label` | `string` | The label for data segmentation |\n",
    "| `accel_x` | `double` | The X-axis accelerometer value |\n",
    "| `accel_y` | `double` | The Y-axis accelerometer value |\n",
    "| `accel_z` | `double` | The Z-axis accelerometer value |\n",
    "| `gyro_roll` | `double` | The Roll-axis accelerometer value |\n",
    "| `gyro_pitch` | `double` | The Pitch-axis accelerometer value |\n",
    "| `gyro_yaw` | `double` | The Yaw-axis accelerometer value |\n",
    "\n",
    "## Schema on Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "csvSchema = StructType([\n",
    "    StructField(\"c0\", StringType(), True),\n",
    "    StructField(\"event_type\", StringType(), False),\n",
    "    StructField(\"label\", StringType(), False),\n",
    "    StructField(\"timestamp\", LongType(), False),\n",
    "    StructField(\"accel_x\", DoubleType(), False),\n",
    "    StructField(\"accel_y\", DoubleType(), False),\n",
    "    StructField(\"accel_z\", DoubleType(), False),\n",
    "    StructField(\"gyro_roll\", DoubleType(), False),\n",
    "    StructField(\"gyro_pitch\", DoubleType(), False),\n",
    "    StructField(\"gyro_yaw\", DoubleType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema on TimescaleDB\n",
    "\n",
    "For the database to serve as a read to visualize and query our truck data, we'll go with TimescaleDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2 in /opt/conda/lib/python3.7/site-packages (2.8.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172.17.0.2\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from config import parse_config\n",
    "from sql_queries import create_jerked_truck_events_table, drop_jerked_truck_events_table\n",
    "\n",
    "configs = parse_config()\n",
    "\n",
    "print(configs['timescaledb']['host'])\n",
    "\n",
    "# connect to recreate the database\n",
    "conn = psycopg2.connect(\"host={} port={} user={} password={}\".format( \\\n",
    "    configs['timescaledb']['host'], \\\n",
    "    configs['timescaledb']['port'], \\\n",
    "    configs['timescaledb']['user'], \\\n",
    "    configs['timescaledb']['password'], \\\n",
    "))\n",
    "conn.set_session(autocommit=True)\n",
    "cur = conn.cursor()\n",
    "\n",
    "database_name = configs['timescaledb']['db']\n",
    "\n",
    "#cur.execute(\"DROP DATABASE IF EXISTS {}\".format(database_name))\n",
    "#cur.execute(\"CREATE DATABASE {} WITH ENCODING 'utf8' TEMPLATE template0\".format(database_name))\n",
    "\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to create the tables in the database\n",
    "conn = psycopg2.connect(\"host={} port={} dbname={} user={} password={}\".format( \\\n",
    "    configs['timescaledb']['host'], \\\n",
    "    configs['timescaledb']['port'], \\\n",
    "    database_name, \\\n",
    "    configs['timescaledb']['user'], \\\n",
    "    configs['timescaledb']['password'] \\\n",
    "))\n",
    "conn.set_session(autocommit=True)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# create sparkify database with UTF8 encoding\n",
    "cur.execute(drop_jerked_truck_events_table)\n",
    "cur.execute(create_jerked_truck_events_table)\n",
    "\n",
    "\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "conn = psycopg2.connect(\"host={} port={} dbname={} user={} password={}\".format( \\\n",
    "    configs['timescaledb']['host'], \\\n",
    "    configs['timescaledb']['port'], \\\n",
    "    database_name, \\\n",
    "    configs['timescaledb']['user'], \\\n",
    "    configs['timescaledb']['password'] \\\n",
    "))\n",
    "conn.set_session(autocommit=True)\n",
    "cur = conn.cursor()\n",
    "\n",
    "results = cur.execute(\"SELECT COUNT(*) FROM jerked_truck_events\")\n",
    "\n",
    "print(results)\n",
    "\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "We will test load the data just to see if the schema is compatible with the stream file source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_timestamp</th>\n",
       "      <th>event_type</th>\n",
       "      <th>label</th>\n",
       "      <th>window</th>\n",
       "      <th>accel_x</th>\n",
       "      <th>accel_y</th>\n",
       "      <th>accel_z</th>\n",
       "      <th>gyro_roll</th>\n",
       "      <th>gyro_pitch</th>\n",
       "      <th>gyro_yaw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-02-14 16:55:14</td>\n",
       "      <td>agressive_bump</td>\n",
       "      <td>1</td>\n",
       "      <td>(2019-02-14 16:55:14, 2019-02-14 16:55:15)</td>\n",
       "      <td>0.022407</td>\n",
       "      <td>0.042624</td>\n",
       "      <td>0.739348</td>\n",
       "      <td>0.004331</td>\n",
       "      <td>0.006097</td>\n",
       "      <td>0.026767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-02-14 16:58:51</td>\n",
       "      <td>agressive_bump</td>\n",
       "      <td>1</td>\n",
       "      <td>(2019-02-14 16:58:51, 2019-02-14 16:58:52)</td>\n",
       "      <td>0.038798</td>\n",
       "      <td>-0.156145</td>\n",
       "      <td>0.753374</td>\n",
       "      <td>0.228012</td>\n",
       "      <td>0.028823</td>\n",
       "      <td>0.093116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-02-14 16:59:24</td>\n",
       "      <td>agressive_bump</td>\n",
       "      <td>0</td>\n",
       "      <td>(2019-02-14 16:59:24, 2019-02-14 16:59:25)</td>\n",
       "      <td>0.135820</td>\n",
       "      <td>0.107514</td>\n",
       "      <td>0.748405</td>\n",
       "      <td>-0.003508</td>\n",
       "      <td>0.003336</td>\n",
       "      <td>-0.038584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-02-08 19:15:46</td>\n",
       "      <td>aggressive_longitudinal_acceleration</td>\n",
       "      <td>0</td>\n",
       "      <td>(2019-02-08 19:15:46, 2019-02-08 19:15:47)</td>\n",
       "      <td>0.086602</td>\n",
       "      <td>0.063249</td>\n",
       "      <td>0.748522</td>\n",
       "      <td>0.004492</td>\n",
       "      <td>0.012335</td>\n",
       "      <td>0.024353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-02-08 19:21:42</td>\n",
       "      <td>aggressive_longitudinal_acceleration</td>\n",
       "      <td>0</td>\n",
       "      <td>(2019-02-08 19:21:42, 2019-02-08 19:21:43)</td>\n",
       "      <td>0.079745</td>\n",
       "      <td>0.072918</td>\n",
       "      <td>0.749915</td>\n",
       "      <td>-0.001045</td>\n",
       "      <td>-0.002719</td>\n",
       "      <td>-0.008712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-02-08 11:29:46</td>\n",
       "      <td>aggressive_turn</td>\n",
       "      <td>1</td>\n",
       "      <td>(2019-02-08 11:29:46, 2019-02-08 11:29:47)</td>\n",
       "      <td>0.073623</td>\n",
       "      <td>-0.156612</td>\n",
       "      <td>0.749980</td>\n",
       "      <td>0.040625</td>\n",
       "      <td>-0.009546</td>\n",
       "      <td>0.448151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-02-11 18:16:21</td>\n",
       "      <td>normal_longitudinal_acceleration</td>\n",
       "      <td>0</td>\n",
       "      <td>(2019-02-11 18:16:21, 2019-02-11 18:16:22)</td>\n",
       "      <td>0.044929</td>\n",
       "      <td>0.089182</td>\n",
       "      <td>0.752091</td>\n",
       "      <td>0.012281</td>\n",
       "      <td>-0.008602</td>\n",
       "      <td>-0.002954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-02-11 18:21:09</td>\n",
       "      <td>normal_longitudinal_acceleration</td>\n",
       "      <td>0</td>\n",
       "      <td>(2019-02-11 18:21:09, 2019-02-11 18:21:10)</td>\n",
       "      <td>0.029329</td>\n",
       "      <td>0.007779</td>\n",
       "      <td>0.750757</td>\n",
       "      <td>-0.006642</td>\n",
       "      <td>-0.000773</td>\n",
       "      <td>0.242208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-02-11 18:22:11</td>\n",
       "      <td>normal_longitudinal_acceleration</td>\n",
       "      <td>0</td>\n",
       "      <td>(2019-02-11 18:22:11, 2019-02-11 18:22:12)</td>\n",
       "      <td>0.043751</td>\n",
       "      <td>0.084993</td>\n",
       "      <td>0.750635</td>\n",
       "      <td>0.000787</td>\n",
       "      <td>-0.002686</td>\n",
       "      <td>-0.049336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-02-11 16:10:11</td>\n",
       "      <td>normal_mixed</td>\n",
       "      <td>0</td>\n",
       "      <td>(2019-02-11 16:10:11, 2019-02-11 16:10:12)</td>\n",
       "      <td>0.096148</td>\n",
       "      <td>-0.045641</td>\n",
       "      <td>0.751135</td>\n",
       "      <td>0.002262</td>\n",
       "      <td>0.001110</td>\n",
       "      <td>0.045268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       date_timestamp                            event_type label  \\\n",
       "0 2019-02-14 16:55:14                        agressive_bump     1   \n",
       "1 2019-02-14 16:58:51                        agressive_bump     1   \n",
       "2 2019-02-14 16:59:24                        agressive_bump     0   \n",
       "3 2019-02-08 19:15:46  aggressive_longitudinal_acceleration     0   \n",
       "4 2019-02-08 19:21:42  aggressive_longitudinal_acceleration     0   \n",
       "5 2019-02-08 11:29:46                       aggressive_turn     1   \n",
       "6 2019-02-11 18:16:21      normal_longitudinal_acceleration     0   \n",
       "7 2019-02-11 18:21:09      normal_longitudinal_acceleration     0   \n",
       "8 2019-02-11 18:22:11      normal_longitudinal_acceleration     0   \n",
       "9 2019-02-11 16:10:11                          normal_mixed     0   \n",
       "\n",
       "                                       window   accel_x   accel_y   accel_z  \\\n",
       "0  (2019-02-14 16:55:14, 2019-02-14 16:55:15)  0.022407  0.042624  0.739348   \n",
       "1  (2019-02-14 16:58:51, 2019-02-14 16:58:52)  0.038798 -0.156145  0.753374   \n",
       "2  (2019-02-14 16:59:24, 2019-02-14 16:59:25)  0.135820  0.107514  0.748405   \n",
       "3  (2019-02-08 19:15:46, 2019-02-08 19:15:47)  0.086602  0.063249  0.748522   \n",
       "4  (2019-02-08 19:21:42, 2019-02-08 19:21:43)  0.079745  0.072918  0.749915   \n",
       "5  (2019-02-08 11:29:46, 2019-02-08 11:29:47)  0.073623 -0.156612  0.749980   \n",
       "6  (2019-02-11 18:16:21, 2019-02-11 18:16:22)  0.044929  0.089182  0.752091   \n",
       "7  (2019-02-11 18:21:09, 2019-02-11 18:21:10)  0.029329  0.007779  0.750757   \n",
       "8  (2019-02-11 18:22:11, 2019-02-11 18:22:12)  0.043751  0.084993  0.750635   \n",
       "9  (2019-02-11 16:10:11, 2019-02-11 16:10:12)  0.096148 -0.045641  0.751135   \n",
       "\n",
       "   gyro_roll  gyro_pitch  gyro_yaw  \n",
       "0   0.004331    0.006097  0.026767  \n",
       "1   0.228012    0.028823  0.093116  \n",
       "2  -0.003508    0.003336 -0.038584  \n",
       "3   0.004492    0.012335  0.024353  \n",
       "4  -0.001045   -0.002719 -0.008712  \n",
       "5   0.040625   -0.009546  0.448151  \n",
       "6   0.012281   -0.008602 -0.002954  \n",
       "7  -0.006642   -0.000773  0.242208  \n",
       "8   0.000787   -0.002686 -0.049336  \n",
       "9   0.002262    0.001110  0.045268  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from pyspark.sql import functions as F\n",
    "# from pyspark.sql.functions import col\n",
    "\n",
    "# truck_events_df = spark.read.schema(csvSchema).csv('data/unified.csv', header=True)\n",
    "# truck_events_df = truck_events_df.withColumn(\n",
    "#     \"date_timestamp\",\n",
    "#     F.to_timestamp(F.from_unixtime(((col(\"timestamp\") / 1000) / 1000), 'yyyy-MM-dd HH:mm:ss.SSS'))\n",
    "# )\n",
    "# truck_events_df = (    \n",
    "#     truck_events_df\n",
    "#         .groupBy(col(\"date_timestamp\"), col(\"event_type\"), col(\"label\"), F.window(col(\"date_timestamp\"), \"1 second\"))\n",
    "#         .agg(\n",
    "#             F.mean('accel_x'),\n",
    "#             F.mean('accel_y'),\n",
    "#             F.mean('accel_z'),\n",
    "#             F.mean('gyro_roll'),\n",
    "#             F.mean('gyro_pitch'),\n",
    "#             F.mean('gyro_yaw')\n",
    "#         )\n",
    "#         .withColumnRenamed('avg(accel_x)', 'accel_x')\n",
    "#         .withColumnRenamed('avg(accel_y)', 'accel_y')\n",
    "#         .withColumnRenamed('avg(accel_z)', 'accel_z')\n",
    "#         .withColumnRenamed('avg(gyro_roll)', 'gyro_roll')\n",
    "#         .withColumnRenamed('avg(gyro_pitch)', 'gyro_pitch')\n",
    "#         .withColumnRenamed('avg(gyro_yaw)', 'gyro_yaw')\n",
    "# )\n",
    "\n",
    "# truck_events_df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Processing\n",
    "\n",
    "Now that we tested the schema by loading our default `data/unified.csv`, we have to set the stream processing options and actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPath = '/home/jovyan/workspace/Projects/truck-data-wrangler/data/'\n",
    "\n",
    "rawRecords = (\n",
    "    spark\n",
    "        .readStream\n",
    "        .schema(csvSchema)\n",
    "        .option(\"maxFilesPerTrigger\", 1)\n",
    "        .csv(inputPath, header=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating jerk data as stream flow in\n",
    "\n",
    "Essentially, we need to be calculating the jerk values and the flags (is_accelerating, is_breaking, is_turning_right and is_turning_left), however the streaming data frame don't support partitioning/ordering windows with non-time based column types. For that reason we will have to explode that columns in another table using the `forEachBatch` callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date_timestamp: timestamp (nullable = true)\n",
      " |-- event_type: string (nullable = false)\n",
      " |-- label: string (nullable = false)\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- accel_x: double (nullable = true)\n",
      " |-- accel_y: double (nullable = true)\n",
      " |-- accel_z: double (nullable = true)\n",
      " |-- gyro_roll: double (nullable = true)\n",
      " |-- gyro_pitch: double (nullable = true)\n",
      " |-- gyro_yaw: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- c0: string (nullable = true)\n",
      " |-- event_type: string (nullable = false)\n",
      " |-- label: string (nullable = false)\n",
      " |-- timestamp: long (nullable = false)\n",
      " |-- accel_x: double (nullable = false)\n",
      " |-- accel_y: double (nullable = false)\n",
      " |-- accel_z: double (nullable = false)\n",
      " |-- gyro_roll: double (nullable = false)\n",
      " |-- gyro_pitch: double (nullable = false)\n",
      " |-- gyro_yaw: double (nullable = false)\n",
      " |-- date_timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "jerk_truck_events_df = rawRecords\n",
    "\n",
    "jerk_truck_events_df = jerk_truck_events_df.withColumn(\n",
    "    \"date_timestamp\",\n",
    "    F.to_timestamp(F.from_unixtime(((col(\"timestamp\") / 1000) / 1000), 'yyyy-MM-dd HH:mm:ss.SSS'))\n",
    ")\n",
    "\n",
    "#column_list = [\"timestamp\", \"event_type\",\"label\"]\n",
    "\n",
    "windowedStreaming = (\n",
    "    jerk_truck_events_df\n",
    "        .groupBy(col(\"date_timestamp\"), col(\"event_type\"), col(\"label\"), F.window(col(\"date_timestamp\"), \"1 second\"))\n",
    "        .agg(\n",
    "            F.mean('accel_x'),\n",
    "            F.mean('accel_y'),\n",
    "            F.mean('accel_z'),\n",
    "            F.mean('gyro_roll'),\n",
    "            F.mean('gyro_pitch'),\n",
    "            F.mean('gyro_yaw')\n",
    "        )\n",
    "        .withColumnRenamed('avg(accel_x)', 'accel_x')\n",
    "        .withColumnRenamed('avg(accel_y)', 'accel_y')\n",
    "        .withColumnRenamed('avg(accel_z)', 'accel_z')\n",
    "        .withColumnRenamed('avg(gyro_roll)', 'gyro_roll')\n",
    "        .withColumnRenamed('avg(gyro_pitch)', 'gyro_pitch')\n",
    "        .withColumnRenamed('avg(gyro_yaw)', 'gyro_yaw')\n",
    ")\n",
    "\n",
    "windowedStreaming.printSchema()\n",
    "\n",
    "jerk_truck_events_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-95d544604d3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"checkpointLocation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".spark-stream-checkpoint/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mforeachBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexplodeJerkColumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "\n",
    "def explodeJerkColumns(df, epochId):\n",
    "    global configs\n",
    "    jerk_truck_events_df = df\n",
    "    \n",
    "    column_list = [\"event_type\",\"label\"]\n",
    "    \n",
    "    jerk_truck_events_df = jerk_truck_events_df.withColumn(\"timestamp\", F.unix_timestamp(col(\"date_timestamp\")))\n",
    "    \n",
    "    win_spec = Window.partitionBy([col(x) for x in column_list]).orderBy(\"timestamp\")\n",
    "\n",
    "    columns_that_needs_latest_values = ['accel_x', 'accel_y', 'accel_z']\n",
    "\n",
    "    for column_name in columns_that_needs_latest_values:\n",
    "        jerk_truck_events_df = jerk_truck_events_df.withColumn(\"last_\" + column_name, F.lag(col(column_name)).over(win_spec))\n",
    "    \n",
    "    # last timestamp before this registry\n",
    "#     jerk_truck_events_df = jerk_truck_events_df.withColumn(\"latest_timestamp\", F.lag(col(\"timestamp\")).over(win_spec))\n",
    "    jerk_truck_events_df = jerk_truck_events_df.withColumn(\n",
    "        \"last_timestamp\", \n",
    "        F.when(F.isnull(F.lag(col(\"date_timestamp\")).over(win_spec)), 0)\n",
    "         .otherwise(F.unix_timestamp(F.lag(col(\"date_timestamp\")).over(win_spec)))\n",
    "    )\n",
    "#     jerk_truck_events_df = jerk_truck_events_df.withColumn(\"date_timestamp\", col(\"timestamp\"))\n",
    "\n",
    "    # x axis\n",
    "    jerk_truck_events_df = jerk_truck_events_df.withColumn(\n",
    "        \"jerk_x\", \n",
    "        F.when(F.isnull(col(\"last_accel_x\")), 0)\n",
    "         .when(F.isnull(col(\"last_timestamp\")), 0)\n",
    "         .otherwise((col(\"accel_x\") - col(\"last_accel_x\")) / (col(\"timestamp\") - col(\"last_timestamp\")))\n",
    "    )\n",
    "\n",
    "    # y axis\n",
    "    jerk_truck_events_df = jerk_truck_events_df.withColumn(\n",
    "        \"jerk_y\", \n",
    "        F.when(F.isnull(col(\"last_accel_y\")), 0)\n",
    "         .when(F.isnull(col(\"last_timestamp\")), 0)\n",
    "         .otherwise((col(\"accel_y\") - col(\"last_accel_y\")) / (col(\"timestamp\") - col(\"last_timestamp\")))\n",
    "    )\n",
    "\n",
    "    # z axis\n",
    "    jerk_truck_events_df = jerk_truck_events_df.withColumn(\n",
    "        \"jerk_z\", \n",
    "        F.when(F.isnull(col(\"last_accel_z\")), 0)\n",
    "         .when(F.isnull(col(\"last_timestamp\")), 0)\n",
    "         .otherwise((col(\"accel_z\") - col(\"last_accel_z\")) / (col(\"timestamp\") - col(\"last_timestamp\")))\n",
    "    )\n",
    "\n",
    "    # adding the is_accelerating flag\n",
    "    jerk_truck_events_df = jerk_truck_events_df.withColumn(\n",
    "        \"is_accelerating\",\n",
    "        F.when(F.isnull(col(\"jerk_x\")), 0)\n",
    "         .when(col(\"jerk_x\") > 0, 1)\n",
    "         .otherwise(0)\n",
    "    )\n",
    "\n",
    "    # adding the is_breaking flag\n",
    "    jerk_truck_events_df = jerk_truck_events_df.withColumn(\n",
    "        \"is_breaking\",\n",
    "        F.when(F.isnull(col(\"jerk_x\")), 0)\n",
    "         .when(col(\"jerk_x\") < 0, 1)\n",
    "         .otherwise(0)\n",
    "    )\n",
    "    \n",
    "    dbhost = configs['timescaledb']['host']\n",
    "    dbport = configs['timescaledb']['port']\n",
    "    dbname = configs['timescaledb']['db']\n",
    "    dbuser = configs['timescaledb']['user']\n",
    "    dbpass = configs['timescaledb']['password']\n",
    "    url = \"jdbc:postgresql://\"+dbhost+\":\"+dbport+\"/\"+dbname\n",
    "    properties = {\n",
    "        \"driver\": \"org.postgresql.Driver\",\n",
    "        \"user\": dbuser,\n",
    "        \"password\": dbpass\n",
    "    }\n",
    "    \n",
    "    jerk_truck_events_df = jerk_truck_events_df[[\n",
    "        'date_timestamp',\n",
    "        'event_type',\n",
    "        'label',\n",
    "        'accel_x',\n",
    "        'accel_y',\n",
    "        'accel_z',\n",
    "        'gyro_roll',\n",
    "        'gyro_pitch',\n",
    "        'gyro_yaw',\n",
    "        'last_timestamp',\n",
    "        'last_accel_x',\n",
    "        'last_accel_y',\n",
    "        'last_accel_z',\n",
    "        'jerk_x',\n",
    "        'jerk_y',\n",
    "        'jerk_z',\n",
    "        'is_accelerating',\n",
    "        'is_breaking'\n",
    "    ]]\n",
    "\n",
    "    jerk_truck_events_df.write.jdbc(url=url, table=\"jerked_truck_events\", mode=\"append\",\n",
    "                          properties=properties)\n",
    "\n",
    "#streamingIn = jerk_truck_events_df \\\n",
    "streamingIn = windowedStreaming \\\n",
    "    .writeStream \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .option(\"checkpointLocation\", \".spark-stream-checkpoint/\") \\\n",
    "    .foreachBatch(explodeJerkColumns) \\\n",
    "    .start(path=inputPath) \\\n",
    "    .awaitTermination()\n",
    "\n",
    "#sleep(30)  # wait a bit for computation to start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#spark.sql(\"SELECT * FROM jerked_truck_events\").limit(10).toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
