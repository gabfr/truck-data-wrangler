{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Truck Data Wrangler\n",
    "\n",
    "Exploratory analysis of the dataset inside the `data` folder.\n",
    "\n",
    "## Unifying the csvÂ´s\n",
    "\n",
    "First of all we will aggregate all csv data inside a single one. All the CSV files has these commons columns:\n",
    "\n",
    " - Timestamp (`timestamp timestamp`)\n",
    " - Longitudinal acceleration (`accel_x decimal (12, 6)`)\n",
    " - Lateral acceleration (`accel_y decimal (12, 6)`)\n",
    " - Vertical acceleration (`accel_z decimal (12, 6)`)\n",
    " - Roll rotation (`gyro_roll decimal (12, 6)`)\n",
    " - Pitch rotation (`gyro_pitch decimal (12, 6)`)\n",
    " - Yaw rotation (`gyro_yaw decimal (12, 6)`)\n",
    " - Label (`label integer`)\n",
    "\n",
    "And we will create another one to identify from which file the rows are coming from:\n",
    "\n",
    " - Event type (`event_type string`)\n",
    " \n",
    "Altought this is not an scalable approach, I will use pandas just for the sake of this specific dataset analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'data/aggressive_bump_1550163148318484.csv' does not exist: b'data/aggressive_bump_1550163148318484.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cce000acc1e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtmp_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mtmp_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'event_type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtmp_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'event_type'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'accel_x'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'accel_y'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'accel_z'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gyro_roll'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gyro_pitch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gyro_yaw'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'data/aggressive_bump_1550163148318484.csv' does not exist: b'data/aggressive_bump_1550163148318484.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_files = {\n",
    "    'aggressive_bump_1550163148318484.csv': 'agressive_bump',\n",
    "    'aggressive_longitudinal_acceleration_1549653321089461.csv': 'aggressive_longitudinal_acceleration',\n",
    "    'aggressive_turn_1549625320507325.csv': 'aggressive_turn',\n",
    "    'normal_longitudinal_acceleration_1549908723215048.csv': 'normal_longitudinal_acceleration',\n",
    "    'normal_mixed_1549901031015048.csv': 'normal_mixed',\n",
    "    'normal_mixed_1550054269957615.csv': 'normal_mixed',\n",
    "    'normal_turn_1549626293857325.csv': 'normal_turn'\n",
    "}\n",
    "\n",
    "read_frames = []\n",
    "\n",
    "for data_file, event_type in data_files.items():\n",
    "    tmp_df = pd.read_csv('raw_data/' + data_file)\n",
    "    tmp_df['event_type'] = event_type\n",
    "    tmp_df = tmp_df[['event_type', 'label', 'timestamp', 'accel_x', 'accel_y', 'accel_z', 'gyro_roll', 'gyro_pitch', 'gyro_yaw']]\n",
    "    read_frames.append(tmp_df)\n",
    "\n",
    "df = pd.concat(read_frames)\n",
    "df.to_csv('data/unified.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Spark to wrangle the data\n",
    "\n",
    "After creating the Spark session, we will load the `data/unified.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Truck Data Wrangler\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average acceleration by event type and label\n",
    "\n",
    "To calculate the average acceleration, grouping by event type and label, we will first read the unified.csv and create a SQL table from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "csvSchema = StructType([\n",
    "    StructField(\"c0\", IntegerType(), True),\n",
    "    StructField(\"event_type\", StringType(), False),\n",
    "    StructField(\"label\", StringType(), False),\n",
    "    StructField(\"timestamp\", LongType(), False),\n",
    "    StructField(\"accel_x\", DoubleType(), False),\n",
    "    StructField(\"accel_y\", DoubleType(), False),\n",
    "    StructField(\"accel_z\", DoubleType(), False),\n",
    "    StructField(\"gyro_roll\", DoubleType(), False),\n",
    "    StructField(\"gyro_pitch\", DoubleType(), False),\n",
    "    StructField(\"gyro_yaw\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "truck_events_df = spark.read.schema(csvSchema).csv('data/unified.csv', header=True)\n",
    "truck_events_df.createOrReplaceTempView(\"truck_events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the SQL view, the following query will give us the grouped results we want to analyze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    event_type, \n",
    "    label, \n",
    "    AVG(accel_x) AS avg_accel_x,\n",
    "    AVG(accel_y) AS avg_accel_y,\n",
    "    AVG(accel_z) AS avg_accel_z\n",
    "FROM \n",
    "    truck_events \n",
    "GROUP BY\n",
    "    event_type, label\n",
    "ORDER BY\n",
    "    event_type, label\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The maximum jerk timestamp\n",
    "\n",
    "To calculate the maximum jerk timestamp, first of all, we have to calculate the jerk of the momentum.\n",
    "\n",
    "Thus, to calculate the jerk we can do it with this simple equation:\n",
    "\n",
    "`jerk = (ð2 â ð1) / (ð¡2 â ð¡1)`\n",
    "\n",
    "Where:\n",
    " - ð2 is the acceleration of the current sensor registry\n",
    " - ð1 is the acceleration of the last sensor registry before the current\n",
    " - ð¡2 is the timestamp of the current sensor registry\n",
    " - ð¡1 is the timestamp of the last sensor registry before the current\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "column_list = [\"event_type\",\"label\"]\n",
    "win_spec = Window.partitionBy([col(x) for x in column_list]).orderBy(\"timestamp\")\n",
    "\n",
    "jerk_truck_events_df = truck_events_df\n",
    "\n",
    "columns_that_needs_latest_values = ['accel_x', 'accel_y', 'accel_z', 'timestamp']\n",
    "\n",
    "for column_name in columns_that_needs_latest_values:\n",
    "    jerk_truck_events_df = jerk_truck_events_df.withColumn(\"last_\" + column_name, F.lag(col(column_name)).over(win_spec))\n",
    "\n",
    "jerk_truck_events_df.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the latest values appended on the data frame, we need to create a last column with the jerk value calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x axis\n",
    "df = jerk_truck_events_df.withColumn(\n",
    "    \"jerk_x\", \n",
    "    F.when(F.isnull(col(\"last_accel_x\")), 0)\n",
    "     .when(F.isnull(col(\"last_timestamp\")), 0)\n",
    "     .otherwise((col(\"accel_x\") - col(\"last_accel_x\")) / (col(\"timestamp\") - col(\"last_timestamp\")))\n",
    ")\n",
    "\n",
    "# y axis\n",
    "df = df.withColumn(\n",
    "    \"jerk_y\", \n",
    "    F.when(F.isnull(col(\"last_accel_y\")), 0)\n",
    "     .when(F.isnull(col(\"last_timestamp\")), 0)\n",
    "     .otherwise((col(\"accel_y\") - col(\"last_accel_y\")) / (col(\"timestamp\") - col(\"last_timestamp\")))\n",
    ")\n",
    "\n",
    "# z axis\n",
    "df = df.withColumn(\n",
    "    \"jerk_z\", \n",
    "    F.when(F.isnull(col(\"last_accel_z\")), 0)\n",
    "     .when(F.isnull(col(\"last_timestamp\")), 0)\n",
    "     .otherwise((col(\"accel_z\") - col(\"last_accel_z\")) / (col(\"timestamp\") - col(\"last_timestamp\")))\n",
    ")\n",
    "\n",
    "#df.toPandas().head()\n",
    "df.describe(\"jerk_x\").show()\n",
    "df.describe(\"jerk_y\").show()\n",
    "df.describe(\"jerk_z\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answering the question, the of the maximum jerk timestamp (per event_type/label):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"jerked_truck_events\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    event_type, \n",
    "    label, \n",
    "    from_unixtime(cast(((timestamp / 1000) / 1000) as bigint),'yyyy-MM-dd HH:mm:ss.SSS') AS timestamp,\n",
    "    jerk_x\n",
    "FROM \n",
    "    jerked_truck_events \n",
    "ORDER BY\n",
    "    jerk_x DESC\n",
    "LIMIT 1\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    event_type, \n",
    "    label, \n",
    "    from_unixtime(cast(((timestamp / 1000) / 1000) as bigint),'yyyy-MM-dd HH:mm:ss.SSS') AS timestamp,\n",
    "    jerk_y\n",
    "FROM \n",
    "    jerked_truck_events \n",
    "ORDER BY\n",
    "    jerk_y DESC, event_type, label\n",
    "LIMIT 1\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    event_type, \n",
    "    label, \n",
    "    from_unixtime(cast(((timestamp / 1000) / 1000) as bigint),'yyyy-MM-dd HH:mm:ss.SSS') AS timestamp,\n",
    "    jerk_z\n",
    "FROM \n",
    "    jerked_truck_events \n",
    "ORDER BY\n",
    "    jerk_z DESC, event_type, label\n",
    "LIMIT 1\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "df_jerk = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    from_unixtime(cast(((timestamp / 1000) / 1000) as bigint),'yyyy-MM-dd HH:mm:ss.SSS') AS timestamp,\n",
    "    event_type, \n",
    "    label,\n",
    "    jerk_x,\n",
    "    jerk_y,\n",
    "    jerk_z\n",
    "FROM \n",
    "    jerked_truck_events \n",
    "ORDER BY\n",
    "    jerked_truck_events.timestamp, event_type, label\n",
    "\"\"\").toPandas()\n",
    "\n",
    "def show_subplots(grouped_df_jerk):\n",
    "    ncols=2\n",
    "    nrows = int(np.ceil(grouped_df_jerk.ngroups/ncols))\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10,20), sharey=True)\n",
    "    \n",
    "    fig.autofmt_xdate()\n",
    "\n",
    "    for (key, ax) in zip(grouped_df_jerk.groups.keys(), axes.flatten()):\n",
    "        ax.fmt_xdata = mdates.DateFormatter('%Y-%m-%d ')\n",
    "        ax.set_title(key)\n",
    "        grouped_df_jerk.get_group(key).plot(kind='bar',ax=ax,x='timestamp')\n",
    "\n",
    "    ax.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "grouped_df_jerk = df_jerk[['timestamp', 'event_type', 'label', 'jerk_x']].groupby(['event_type', 'label'])\n",
    "\n",
    "show_subplots(grouped_df_jerk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df_jerk = df_jerk[['timestamp', 'event_type', 'label', 'jerk_y']].groupby(['event_type', 'label'])\n",
    "\n",
    "show_subplots(grouped_df_jerk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df_jerk = df_jerk[['timestamp', 'event_type', 'label', 'jerk_z']].groupby(['event_type', 'label'])\n",
    "\n",
    "show_subplots(grouped_df_jerk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
